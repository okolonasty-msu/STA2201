
---
title: "EDA and data visualization"
author: "Monica Alexander"
date: "January 18 2022"
output: 
    pdf_document:
      number_sections: true
      toc: true
      latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Overview

This week we will be going through some exploratory data analysis (EDA) and data visualization steps in R. The aim is to get you used to working with real data (that has issues) to understand the main characteristics and potential issues. 

We will be using the [`opendatatoronto`](https://sharlagelfand.github.io/opendatatoronto/) R package, which interfaces with the City of Toronto Open Data Portal. 

A good resource is part 1 (especially chapters 3 and 7) of 'R for Data Science' by Hadley Wickham, available for free here: https://r4ds.had.co.nz/. 

## What to hand in via GitHub

**There are exercises at the end of this lab**. Please make a new .Rmd file with your answers, call it something sensible (e.g. `week_2_lab.Rmd`), commit to your git repo from last week, and push to GitHub. Due on Friday by 9am. 

## A further note on RStudio projects

I mentioned projects last week, but I'd just like to remind everyone that they're a good thing to use.  The main advantage of projects is that you don't have to set the working directory or type the whole file path to read in a file (for example, a data file). So instead of reading a csv from `"~/Documents/applied-stats/data/"` you can just read it in from `data/`. 

**This is super useful for your assignments**. For the assignments:

- You are expected to hand in the Rmd files, and I should be able to compile these with no errors
- I cannot read in a file that has a path from your local machine. 

## A note on packages

If you are running this Rmd on your local machine, you may need to install various packages used (using the `install.packages` function). 
Load in all the packages we need:

```{r}
#install.packages('opendatatoronto')
library(opendatatoronto)
library(tidyverse)
library(stringr)
#install.packages('skimr')
library(skimr) # EDA
#install.packages('visdat')
library(visdat) # EDA
#install.packages('janitor')
library(janitor)
library(lubridate)
#install.packages('ggrepel')
library(ggrepel)
```


# TTC subway delays 

This package provides an interface to all data available on the [Open Data Portal](https://open.toronto.ca/) provided by the City of Toronto. 

Use the `list_packages` function to look whats available look at what's available


```{r}
all_data <- list_packages(limit = 500)
head(all_data)
```

Let's download the data on TTC subway delays in 2021. There are multiple files for 2021. so we need to get them all and make them into one big dataframe. 


```{r}
res <- list_package_resources("996cfe8d-fb35-40ce-b569-698d51fc683b") # obtained code from searching data frame above
res <- res %>% mutate(year = str_extract(name, "202.?"))
delay_2021_ids <- res %>% filter(year==2021) %>% select(id) %>% pull()
delay_2021 <- c()
for(i in 1:length(delay_2021_ids)) {
  delay_2021 <- bind_rows(delay_2021, get_resource(delay_2021_ids[i]))
}
# make the column names nicer to work with
delay_2021 <- clean_names(delay_2021)
```
Let's also download the delay code and readme, as reference. 

```{r}
# note: I obtained these codes from the 'id' column in the `res` object above
delay_codes <- get_resource("fece136b-224a-412a-b191-8d31eb00491e")
delay_data_codebook <- get_resource("54247e39-5a7d-40db-a137-82b2a9ab0708")
```

This dataset has a bunch of interesting variables. You can refer to the readme for descriptions. Our outcome of interest is `min_delay`, which give the delay in mins. 

```{r}
head(delay_2021)
```
# EDA and data viz

The following section highlights some tools that might be useful for you when you are getting used to a new dataset. There's no one way of exploration, but it's important to always keep in mind: 

- what should your variables look like (type, values, distribution, etc)
- what would be surprising (outliers etc)
- what is your end goal (here, it might be understanding factors associated with delays, e.g. stations, time of year, time of day, etc)

In any data analysis project, if it turns out you have data issues, surprising values, missing data etc, it's important you **document** anything you found and the subsequent steps or **assumptions** you made before moving onto your data analysis / modeling. 


## Data checks

### Sanity Checks

We need to check variables should be what they say they are. If they aren't, the natural next question is to what to do with issues (recode? remove?)

E.g. check days of week 

```{r}
unique(delay_2021$day)
```

Check lines: oh no. some issues here. Some have obvious recodes, others, not so much. 

```{r}
unique(delay_2021$line)
```

The `skimr` package might also be useful here

```{r}
skim(delay_2021)
```

What are the different values of `bound` for each `line`?

For simplicity, just keep the correct line labels.

```{r}
delay_2021 %>%
  filter(line %in% c("BD", "YU", "SHP", "SRT")) %>%
  mutate(bound = as.factor(bound)) %>%
  group_by(line) %>%
  skim(bound)
```


### Missing values

The `visdat` package is useful here, particularly to see how missing values are distributed. 

```{r}
vis_dat(delay_2021)
vis_miss(delay_2021)
```

### Duplicates?

The `get_dupes` function from the `janitor` package is useful for this. 

```{r}
get_dupes(delay_2021)
```

There are quite a few duplicates. Remove for now:

```{r}
delay_2021 <- delay_2021 %>% distinct()
```



## Visualizing distributions

Histograms, barplots, and density plots are your friends here. 

Let's look at the outcome of interest: `min_delay`. First of all just a histogram of all the data:

```{r}
## Removing the observations that have non-standardized lines
delay_2021 <- delay_2021 %>% filter(line %in% c("BD", "YU", "SHP", "SRT"))
ggplot(data = delay_2021) + 
  geom_histogram(aes(x = min_delay))
```

To improve readability, could plot on logged scale:

```{r}
ggplot(data = delay_2021) + 
  geom_histogram(aes(x = min_delay)) + 
  scale_x_log10()
```


Our initial EDA hinted at an outlying delay time, let's take a look at the largest delays below. Join the `delay_codes` dataset to see what the delay is. (Have to do some mangling as SRT has different codes).

```{r}
delay_2021 <- delay_2021 %>% 
  left_join(delay_codes %>% rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %>% select(code, code_desc)) 
delay_2021 <- delay_2021 %>%
  mutate(code_srt = ifelse(line=="SRT", code, "NA")) %>% 
  left_join(delay_codes %>% rename(code_srt = `SRT RMENU CODE`, code_desc_srt = `CODE DESCRIPTION...7`) %>% select(code_srt, code_desc_srt))  %>% 
  mutate(code = ifelse(code_srt=="NA", code, code_srt),
         code_desc = ifelse(is.na(code_desc_srt), code_desc, code_desc_srt)) %>% 
  select(-code_srt, -code_desc_srt)
```

The largest delay is due to "Traction Power Rail Related".

```{r}
delay_2021 %>% 
  left_join(delay_codes %>% rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %>% select(code, code_desc)) %>% 
  arrange(-min_delay) %>% 
  select(date, time, station, line, min_delay, code, code_desc)
```

#### Grouping and small multiples

A quick and powerful visualization technique is to group the data by a variable of interest, e.g. `line`

```{r}
ggplot(data = delay_2021) + 
  geom_histogram(aes(x = min_delay, y = ..density.., fill = line), position = 'dodge', bins = 10) +
  scale_x_log10()
```

I switched to density above to look at the the distributions more comparably, but we should also be aware of differences in frequency, in particular, SHP and SRT have much smaller counts:

```{r}
ggplot(data = delay_2021) + 
  geom_histogram(aes(x = min_delay, fill = line), position = 'dodge', bins = 10) + 
  scale_x_log10()
```



If you want to group by more than one variable, facets are good:

```{r}
ggplot(data = delay_2021) + 
  geom_density(aes(x = min_delay, color = day), bw = .08) + 
  scale_x_log10() + 
  facet_wrap(~line)
```

Side note: the station names are a mess. Try and clean up the station names a bit by taking just the first word (or, the first two if it starts with "ST"):

```{r}
delay_2021 <- delay_2021 %>% 
  mutate(station_clean = ifelse(str_starts(station, "ST"), word(station, 1,2), word(station, 1)))
```

Plot top five stations by mean delay:

```{r}
delay_2021 %>% 
  group_by(line, station_clean) %>% 
  summarise(mean_delay = mean(min_delay), n_obs = n()) %>% 
  filter(n_obs>1) %>% 
  arrange(line, -mean_delay) %>% 
  slice(1:5) %>% 
  ggplot(aes(station_clean, mean_delay)) + 
  geom_col() + 
  coord_flip() + 
  facet_wrap(~line, scales = "free_y")
```

## Visualizing time series

Daily plot is messy (you can check for yourself). Let's look by week to see if there's any seasonality. The `lubridate` package has lots of helpful functions that deal with date variables. First, mean delay (of those that were delayed more than 0 mins):


```{r}
delay_2021 %>% 
  filter(min_delay>0) %>% 
  mutate(week = week(date)) %>% 
  group_by(week, line) %>% 
  summarise(mean_delay = mean(min_delay)) %>% 
  ggplot(aes(week, mean_delay, color = line)) + 
  geom_point() + 
  geom_smooth() + 
  facet_grid(~line)
```

What about proportion of delays that were greater than 10 mins?

```{r}
delay_2021 %>% 
  mutate(week = week(date)) %>% 
  group_by(week, line) %>% 
  summarise(prop_delay = sum(min_delay>10)/n()) %>% 
  ggplot(aes(week, prop_delay, color = line)) + 
  geom_point() + 
  geom_smooth() + 
  facet_grid(~line)
```

## Visualizing relationships

Note that **scatter plots** are a good precursor to modeling, to visualize relationships between continuous variables. Nothing obvious to plot here, but easy to do with `geom_point`.

Look at top five reasons for delay by station. Do they differ? Think about how this could be modeled. 

```{r}
delay_2021 %>%
  group_by(line, code_desc) %>%
  summarise(mean_delay = mean(min_delay)) %>%
  arrange(-mean_delay) %>%
  slice(1:5) %>%
  ggplot(aes(x = code_desc,
             y = mean_delay)) +
  geom_col() + 
  facet_wrap(vars(line), 
             scales = "free_y",
             nrow = 4) +
  coord_flip()
```

## PCA (additional)
Principal components analysis is a really powerful exploratory tool, particularly when you have a lot of variables. It allows you to pick up potential clusters and/or outliers that can help to inform model building. 

Let's do a quick (and imperfect) example looking at types of delays by station. 

The delay categories are a bit of a mess, and there's hundreds of them. As a simple start, let's just take the first word: 

```{r}
delay_2021 <- delay_2021 %>% 
  mutate(code_red = case_when(
    str_starts(code_desc, "No") ~ word(code_desc, 1, 2),
    str_starts(code_desc, "Operator") ~ word(code_desc, 1,2),
    TRUE ~ word(code_desc,1))
         )
```

Let's also just restrict the analysis to causes that happen at least 50 times over 2021 To do the PCA, the dataframe also needs to be switched to wide format:

```{r}
dwide <- delay_2021 %>% 
  group_by(line, station_clean) %>% 
  mutate(n_obs = n()) %>% 
  filter(n_obs>1) %>% 
  group_by(code_red) %>% 
  mutate(tot_delay = n()) %>% 
  arrange(tot_delay) %>% 
  filter(tot_delay>50) %>% 
  group_by(line, station_clean, code_red) %>% 
  summarise(n_delay = n()) %>% 
  pivot_wider(names_from = code_red, values_from = n_delay) %>% 
  mutate_all(.funs = funs(ifelse(is.na(.), 0, .)))
```

Do the PCA:

```{r}
delay_pca <- prcomp(dwide[,3:ncol(dwide)])
df_out <- as_tibble(delay_pca$x)
df_out <- bind_cols(dwide %>% select(line, station_clean), df_out)
head(df_out)
```


Plot the first two PCs, and label some outlying stations:

```{r}
ggplot(df_out,aes(x=PC1,y=PC2,color=line )) + geom_point() + geom_text_repel(data = df_out %>% filter(PC2>100|PC1<100*-1), aes(label = station_clean))
```

Plot the factor loadings. Some evidence of public v operator?

```{r}
df_out_r <- as_tibble(delay_pca$rotation)
df_out_r$feature <- colnames(dwide[,3:ncol(dwide)])
df_out_r
ggplot(df_out_r,aes(x=PC1,y=PC2,label=feature )) + geom_text_repel()
```

# Lab Exercises

1. Using the `opendatatoronto` package, download the data on mayoral campaign contributions for 2014. Hints:
    + find the ID code you need for the package you need by searching for 'campaign' in the `all_data` tibble above
    + you will then need to `list_package_resources` to get ID for the data file
    + note: the 2014 file you will get from `get_resource` has a bunch of different campaign contributions, so just keep the data that relates to the Mayor election

```{r}
res_1 <- list_package_resources("f6651a40-2f52-46fc-9e04-b760c16edd5c")
head(res_1)
```
```{r}
res_2 <- get_resource('d99bb1f3-949a-4497-bb96-c93bbd203130')
res_df <- as_data_frame(res_2[["2_Mayor_Contributions_2014_election.xls"]])
```
2. Clean up the data format (fixing the parsing issue and standardizing the column names using `janitor`)

```{r}
res_df <- res_df %>%
  row_to_names(row_number = 1)
head(res_df)
```

3. Summarize the variables in the dataset. Are there missing values, and if so, should we be worried about them? Is every variable in the format it should be? If not, create new variable(s) that are in the right format.

```{r}
skim(res_df)
```

I think despite some features are missed, we can use this data as we have almost all general information (postal code as a proxy for a address), all names of contributors and their amounts.

But all our variables are in Character format.

```{r}
res_df_1 <- res_df %>%
  rename(Contribution_Amount = 'Contribution Amount',
         Relationship_to_Candidate = 'Relationship to Candidate',
         Contributors_Name = "Contributor's Name") %>%
  mutate(Contribution_Amount = as.numeric(Contribution_Amount)) 
```

4. Visually explore the distribution of values of the contributions. 

```{r}
ggplot(data = res_df_1) + 
  geom_histogram(aes(x = Contribution_Amount), color = 'White', fill='darkolivegreen1')+
  theme_dark()+
  ggtitle('Countributions, uncleaned')
```

```{r}
res_df_1 %>%
  filter(Contribution_Amount > 10**5)
```
```{r}
unique(res_df_1$Relationship_to_Candidate)
```
This outlier looks like a candidate himself!

Lets take a look on all similar (spouse or candidate) contributions.

```{r}
res_df_plot_outliers <- res_df_1 %>%
  filter(Relationship_to_Candidate == 'Candidate' | Relationship_to_Candidate == 'Spouse')
ggplot(data = res_df_plot_outliers) + 
  geom_histogram(aes(x = Contribution_Amount), color = 'White', fill='pink')+
  theme_dark()+
  ggtitle('Countributions from spouses and candidates themselves')
```

We have one outlier which makes our histogram useless, so I'll plot the hist without this extremal and exlude all spouses' and candidates' contributions.


```{r}
res_df_plot <- res_df_1 %>%
  filter(is.na(Relationship_to_Candidate))

ggplot(data = res_df_plot) + 
  geom_histogram(aes(x = Contribution_Amount), color = 'White', fill='aquamarine')+
  theme_dark()+
  ggtitle('Contributions without outliers')
```


What contributions are notable outliers? Do they share a similar characteristic(s)? 

Candidates itself and their spouses, let's take a look on contributions > 2500 also.

```{r}
res_df_plot_outliers_2 <- res_df_1 %>%
  filter(Contribution_Amount > 2500 & is.na(Relationship_to_Candidate))
#ggplot(data = res_df_plot_outliers_2) + 
  #geom_histogram(aes(x = Contribution_Amount), color = 'White', fill='pink')+
  #theme_dark()
res_df_plot_outliers_2
```
photography :):):)

It may be useful to plot the distribution of contributions without these outliers to get a better sense of the majority of the data. 

Once again:
```{r}
ggplot(data = res_df_plot) + 
  geom_histogram(aes(x = Contribution_Amount), color = 'White', fill='aquamarine')+
  theme_dark()+
  ggtitle('Contributions without outliers')
```


5. List the top five candidates in each of these categories:
    + total contributions
    + mean contribution
    + number of contributions

#Top 5 total contributions    
```{r}
top_5_tc <- res_df_1 %>%
  group_by(Candidate) %>%
  summarise(total_contr = sum(Contribution_Amount)) 

top_5_tc %>%
  top_n(5, total_contr) %>%
  arrange(desc(total_contr))
```

#Top 5 mean contributions 
```{r}
top_5_mc <- res_df_1 %>%
  group_by(Candidate) %>%
  summarise(mean_contr = mean(Contribution_Amount)) 

top_5_mc %>%
  top_n(5, mean_contr) %>%
  arrange(desc(mean_contr))
```

#Top 5 number of contributions 
```{r}
top_5_cc <- res_df_1 %>%
  group_by(Candidate) %>%
  summarise(count_contr = n()) 

top_5_cc %>%
  top_n(5, count_contr) %>%
  arrange(desc(count_contr))
```    


6. Repeat 5 but without contributions from the candidates themselves.

```{r}
res_df_6 <- res_df_1 %>%
  filter(is.na(Relationship_to_Candidate))
```

#Top 5 total contributions    
```{r}
top_5_tc_6 <- res_df_6 %>%
  group_by(Candidate) %>%
  summarise(total_contr = sum(Contribution_Amount)) 

top_5_tc_6 %>%
  top_n(5, total_contr) %>%
  arrange(desc(total_contr))
```
#Top 5 mean contributions 
```{r}
top_5_mc_6 <- res_df_6 %>%
  group_by(Candidate) %>%
  summarise(mean_contr = mean(Contribution_Amount)) 

top_5_mc_6 %>%
  top_n(5, mean_contr) %>%
  arrange(desc(mean_contr))
```

#Top 5 number of contributions 
```{r}
top_5_cc_6 <- res_df_6 %>%
  group_by(Candidate) %>%
  summarise(count_contr = n()) 

top_5_cc_6 %>%
  top_n(5, count_contr) %>%
  arrange(desc(count_contr))
```

7. How many contributors gave money to more than one candidate? 

```{r}
res_df_7 <- res_df_1 %>%
  group_by(Contributors_Name) %>%
  summarise(uni_can = unique(Candidate))

res_df_7 <- res_df_7 %>%
  group_by(Contributors_Name) %>%
  summarise(cnt_diff_cnd = n())

res_df_7 <- res_df_7 %>%
  filter(cnt_diff_cnd > 1)

res_df_7
```

```{r}
#184 rows = 184 contributors
nrow(res_df_7)
```






